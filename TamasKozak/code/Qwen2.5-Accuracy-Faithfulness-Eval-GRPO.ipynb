{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1TB-5VMKipnINXd24DRImYiz0EC2X_oVW","timestamp":1750256407573}],"gpuType":"A100","authorship_tag":"ABX9TyPbO9hjO1io8m9dPGbAq0ka"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth\n","else:\n","    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n","    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n","    !pip install --no-deps unsloth\n","    !pip install wandb"],"metadata":{"id":"8bWr-0G2ppG-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Accuracy + Faithfulness\n","\n","*   Greedy Accuracy - Correctness using single generation vs ground truth, simulating deterministic decoding (0.0 to 1.0)\n","*   Self-Consistency Accuracy - Correctness using majority vote from multiple generations vs ground truth (0.0 to 1.0)\n","*   Consistency Ratio - Frequency of the most common answer across multiple generations (0.0 to 1.0)\n","*   NLI Faithfulness - How well the reasoning supports the final answer using NLI (0.0 to 1.0)"],"metadata":{"id":"4XtLGIW3oDd4"}},{"cell_type":"code","source":["import os\n","import re\n","import torch\n","import random\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from collections import Counter\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from unsloth import FastLanguageModel\n","\n","# Load DPO Model\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=\"/content/checkpoint-114\", # Upload the checkpoint folder to the files\n","    max_seq_length=1024,\n","    dtype=None,\n","    load_in_4bit=True,\n",")\n","FastLanguageModel.for_inference(model)\n","\n","# Load NLI Model\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","nli_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\").eval().to(device)\n","nli_tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n","\n","# Prompt Template\n","reasoning_start = \"<start_working_out>\"\n","reasoning_end   = \"<end_working_out>\"\n","solution_start  = \"<SOLUTION>\"\n","solution_end    = \"</SOLUTION>\"\n","\n","SYSTEM_PROMPT = f\"\"\"You are given a problem.\n","Think about the problem and provide your working out.\n","Place it between {reasoning_start} and {reasoning_end}.\n","Then, provide your solution between {solution_start}{solution_end}\"\"\"\n","\n","def format_prompt(question: str) -> str:\n","    return (\n","        f\"<|system|>\\n{SYSTEM_PROMPT}\\n\"\n","        f\"<|user|>\\n{question}\\n\"\n","        f\"<|assistant|>\\nLet me work through this problem.\\n<start_working_out>\\n\"\n","    )"],"metadata":{"id":"9jybCOQhoDEt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extraction functions\n","def extract_reasoning(text: str) -> str:\n","    try:\n","        blocks = re.findall(r\"<start_working_out>([\\s\\S]*?)<end_working_out>\", text, re.IGNORECASE)\n","        return blocks[-1].strip() if blocks else \"\"\n","    except:\n","        return \"\"\n","\n","def extract_numeric_answer(text: str) -> str:\n","    try:\n","        solution_blocks = re.findall(r\"<SOLUTION>([\\s\\S]*?)</SOLUTION>\", text, re.IGNORECASE)\n","        if solution_blocks:\n","            content = solution_blocks[-1].strip()\n","            match = re.search(r\"-?\\d+(?:\\.\\d+)?\", content.replace(\",\", \"\"))\n","            if match:\n","                return match.group(0)\n","\n","        return None\n","    except:\n","        return None\n","\n","def extract_hash_answer(text: str) -> str:\n","    \"\"\"Extract answer after '#### X' marker in ground truth\"\"\"\n","    if not isinstance(text, str) or \"####\" not in text:\n","        return None\n","\n","    parts = text.split(\"####\")\n","    if len(parts) < 2:\n","        return None\n","\n","    answer_text = parts[1].strip()\n","    answer_text = answer_text.replace(\"$\", \"\").replace(\",\", \"\")\n","\n","    match = re.search(r\"-?\\d+(?:\\.\\d+)?\", answer_text)\n","    return match.group(0) if match else None"],"metadata":{"id":"zQROiVa6o4M_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_multiple_answers_batch(question: str, num_samples: int = 3, temperature: float = 0.7):\n","    prompt = format_prompt(question)\n","\n","    # Prepare batch inputs\n","    batch_inputs = tokenizer([prompt] * num_samples, return_tensors=\"pt\", padding=True).to(model.device)\n","\n","    # Set seeds for each sample in the batch\n","    seeds = [random.randrange(2**32) for _ in range(num_samples)]\n","\n","    results = []\n","    try:\n","        # Generate all samples in one batch call\n","        with torch.no_grad():\n","            output_ids = model.generate(\n","                **batch_inputs,\n","                max_new_tokens=256,\n","                temperature=temperature,\n","                do_sample=True,\n","                top_p=0.95,\n","                pad_token_id=tokenizer.eos_token_id or tokenizer.pad_token_id,\n","            )\n","\n","        # Process each output\n","        for i in range(num_samples):\n","            decoded = tokenizer.decode(output_ids[i], skip_special_tokens=True)\n","            response = decoded.split(\"<|assistant|>\")[-1].strip()\n","\n","            answer = extract_numeric_answer(response)\n","            reasoning = extract_reasoning(response)\n","\n","            results.append((answer, reasoning))\n","\n","    except Exception as e:\n","        print(f\"Batch generation failed: {e}\")\n","        # Fallback to individual generation\n","        return generate_multiple_answers_fallback(question, num_samples, temperature)\n","\n","    return results\n","\n","def generate_multiple_answers_fallback(question: str, num_samples: int = 3, temperature: float = 0.7):\n","    prompt = format_prompt(question)\n","    results = []\n","\n","    for i in range(num_samples):\n","        try:\n","            seed = random.randrange(2**32)\n","            torch.manual_seed(seed)\n","\n","            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","            with torch.no_grad():\n","                output_ids = model.generate(\n","                    **inputs,\n","                    max_new_tokens=256,\n","                    temperature=temperature,\n","                    do_sample=True,\n","                    top_p=0.95,\n","                    pad_token_id=tokenizer.eos_token_id or tokenizer.pad_token_id,\n","                )\n","\n","            decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","            response = decoded.split(\"<|assistant|>\")[-1].strip()\n","\n","            answer = extract_numeric_answer(response)\n","            reasoning = extract_reasoning(response)\n","\n","            results.append((answer, reasoning))\n","\n","        except Exception as e:\n","            print(f\"Generation {i+1} failed: {e}\")\n","            results.append((None, \"\"))\n","\n","    return results"],"metadata":{"id":"W_ciNkcQo9Rn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Metric calculations\n","def get_majority_answer(results):\n","    valid_results = [(ans, reasoning) for ans, reasoning in results if ans is not None]\n","\n","    if not valid_results:\n","        return None\n","\n","    answers = [ans for ans, _ in valid_results]\n","    answer_counts = Counter(answers)\n","    majority_answer, majority_count = answer_counts.most_common(1)[0]\n","\n","    return majority_answer\n","\n","def check_accuracy(pred_answer: str, ground_truth: str) -> bool:\n","    if pred_answer is None or ground_truth is None:\n","        return False\n","\n","    try:\n","        pred_num = float(pred_answer)\n","        gt_num = float(ground_truth)\n","        return abs(pred_num - gt_num) < 1e-6  # Floating point precision\n","    except ValueError:\n","        return pred_answer.strip() == ground_truth.strip()\n","\n","def calculate_consistency_ratio(results):\n","    valid_results = [(ans, reasoning) for ans, reasoning in results if ans is not None]\n","\n","    if not valid_results:\n","        return 0.0\n","\n","    answers = [ans for ans, _ in valid_results]\n","    answer_counts = Counter(answers)\n","    majority_count = answer_counts.most_common(1)[0][1]\n","\n","    return majority_count / len(valid_results)\n","\n","def calculate_nli_faithfulness(reasoning: str, answer: str) -> float:\n","    try:\n","        if not reasoning or not answer:\n","            return 0.0\n","\n","        hypothesis = f\"The final answer is {answer}.\"\n","        premise = reasoning\n","\n","        inputs = nli_tokenizer(\n","            premise, hypothesis,\n","            return_tensors=\"pt\",\n","            truncation=True,\n","            max_length=512\n","        ).to(nli_model.device)\n","\n","        with torch.no_grad():\n","            logits = nli_model(**inputs).logits\n","            probs = logits.softmax(dim=-1)\n","\n","        return probs[0][2].item()  # Entailment probability\n","\n","    except:\n","        return 0.0"],"metadata":{"id":"3d36BYp1pFSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_dataset_batched(df, num_samples=3, temperature=0.7, question_batch_size=4):\n","    consistency_ratios = []\n","    nli_scores = []\n","    sc_accuracy_scores = []\n","    greedy_accuracy_scores = []\n","\n","    # Process multiple questions at once\n","    for batch_start in tqdm(range(0, len(df), question_batch_size)):\n","        batch_end = min(batch_start + question_batch_size, len(df))\n","        batch_questions = []\n","        batch_ground_truths = []\n","\n","        for i in range(batch_start, batch_end):\n","            sample = df.iloc[i]\n","            batch_questions.append(sample[\"question\"])\n","            batch_ground_truths.append(extract_hash_answer(sample[\"answer\"]))\n","\n","        all_results = generate_batch_questions(batch_questions, num_samples, temperature)\n","\n","        for question_results, ground_truth in zip(all_results, batch_ground_truths):\n","            # Self-Consistency\n","            majority_answer = get_majority_answer(question_results)\n","\n","            # Greedy simulation\n","            first_answer = question_results[0][0] if question_results else None\n","\n","            # Consistency Ratio\n","            consistency_ratio = calculate_consistency_ratio(question_results)\n","            consistency_ratios.append(consistency_ratio)\n","\n","            # Self-Consistency Accuracy\n","            sc_is_correct = check_accuracy(majority_answer, ground_truth)\n","            sc_accuracy_scores.append(1.0 if sc_is_correct else 0.0)\n","\n","            # Greedy Accuracy\n","            greedy_is_correct = check_accuracy(first_answer, ground_truth)\n","            greedy_accuracy_scores.append(1.0 if greedy_is_correct else 0.0)\n","\n","            # NLI Faithfulness\n","            nli_score = 0.0\n","            if majority_answer:\n","                for ans, reasoning in question_results:\n","                    if ans == majority_answer and reasoning:\n","                        nli_score = calculate_nli_faithfulness(reasoning, majority_answer)\n","                        break\n","            nli_scores.append(nli_score)\n","\n","    return consistency_ratios, nli_scores, sc_accuracy_scores, greedy_accuracy_scores\n","\n","gsm8k = pd.read_parquet(\"/content/test-00000-of-00001.parquet\") # GSM8K test set downloaded as parquet\n","test_data = gsm8k.sample(n=200, random_state=42).reset_index(drop=True)\n","\n","import time\n","start_time = time.time()\n","\n","consistency_ratios, nli_scores, sc_accuracy_scores, greedy_accuracy_scores = evaluate_dataset_batched(\n","    test_data,\n","    num_samples=3,\n","    temperature=0.7,\n","    question_batch_size=32  # Process 4 questions at once (4 × 3 = 12 generations per batch)\n",")\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","elapsed_minutes = elapsed_time / 60\n","\n","print(\"📊 FINAL RESULTS\")\n","print(f\"Consistency Ratio: {np.mean(consistency_ratios):.3f}\")\n","print(f\"NLI Faithfulness: {np.mean(nli_scores):.3f}\")\n","print(f\"Self-Consistency Accuracy: {np.mean(sc_accuracy_scores):.3f}\")\n","print(f\"Greedy Accuracy: {np.mean(greedy_accuracy_scores):.3f}\")\n","print(f\"Evaluation Time: {elapsed_minutes:.1f} minutes\")"],"metadata":{"id":"jjhTwokD9REV"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNp6FV/UxjAAWm9UwgtwuNn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth\n","else:\n","    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n","    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n","    !pip install --no-deps unsloth\n","    !pip install wandb"],"metadata":{"id":"8bWr-0G2ppG-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Accuracy + Faithfulness\n","\n","*   Greedy Accuracy - Correctness using single generation vs ground truth, simulating deterministic decoding (0.0 to 1.0)\n","*   Self-Consistency Accuracy - Correctness using majority vote from multiple generations vs ground truth (0.0 to 1.0)\n","*   Consistency Ratio - Frequency of the most common answer across multiple generations (0.0 to 1.0)\n","*   NLI Faithfulness - How well the reasoning supports the final answer using NLI (0.0 to 1.0)\n"],"metadata":{"id":"4XtLGIW3oDd4"}},{"cell_type":"code","source":["import os\n","import re\n","import torch\n","import random\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from collections import Counter\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from unsloth import FastLanguageModel\n","\n","# Load DPO Model\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=\"/content/checkpoint-411\", # Upload the checkpoint folder to the files\n","    max_seq_length=1024,\n","    dtype=None,\n","    load_in_4bit=True,\n",")\n","FastLanguageModel.for_inference(model)\n","\n","# Load NLI Model\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","nli_model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\").eval().to(device)\n","nli_tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n","\n","# Prompt Template\n","SYSTEM_PROMPT = \"\"\"You are a precise arithmetic assistant.\n","For each question, output exactly:\n","\n","<reasoning>\n","…your step-by-step operations…\n","</reasoning>\n","<answer>\n","…your final numeric answer…\n","</answer>\"\"\"\n","\n","def format_prompt(question: str) -> str:\n","    return (\n","        f\"<|system|>\\n{SYSTEM_PROMPT}\\n\"\n","        f\"<|user|>\\n{question}\\n\"\n","        f\"<|assistant|>\\nLet's think step by step.\\n<reasoning>\\n\"\n","    )"],"metadata":{"id":"9jybCOQhoDEt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extraction functions\n","def extract_reasoning(text: str) -> str:\n","    try:\n","        blocks = re.findall(r\"<reasoning>([\\s\\S]*?)</reasoning>\", text, re.IGNORECASE)\n","        return blocks[-1].strip() if blocks else \"\"\n","    except:\n","        return \"\"\n","\n","def extract_numeric_answer(text: str) -> str:\n","    try:\n","        # XML format\n","        xml_blocks = re.findall(r\"<answer>([\\s\\S]*?)</answer>\", text, re.IGNORECASE)\n","        if xml_blocks:\n","            content = xml_blocks[-1].strip()\n","            match = re.search(r\"-?\\d+(?:\\.\\d+)?\", content.replace(\",\", \"\"))\n","            if match:\n","                return match.group(0)\n","\n","        # Hash format\n","        if \"####\" in text:\n","            hash_content = text.split(\"####\")[1].strip()\n","            match = re.search(r\"-?\\d+(?:\\.\\d+)?\", hash_content.replace(\",\", \"\"))\n","            if match:\n","                return match.group(0)\n","\n","        return None\n","    except:\n","        return None\n","\n","def extract_hash_answer(text: str) -> str:\n","    \"\"\"Extract answer after '#### X' marker in ground truth\"\"\"\n","    if not isinstance(text, str) or \"####\" not in text:\n","        return None\n","\n","    parts = text.split(\"####\")\n","    if len(parts) < 2:\n","        return None\n","\n","    answer_text = parts[1].strip()\n","    answer_text = answer_text.replace(\"$\", \"\").replace(\",\", \"\")\n","\n","    match = re.search(r\"-?\\d+(?:\\.\\d+)?\", answer_text)\n","    return match.group(0) if match else None"],"metadata":{"id":"zQROiVa6o4M_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_multiple_answers_batch(question: str, num_samples: int = 3, temperature: float = 0.7):\n","    prompt = format_prompt(question)\n","\n","    # Prepare batch inputs\n","    batch_inputs = tokenizer([prompt] * num_samples, return_tensors=\"pt\", padding=True).to(model.device)\n","\n","    # Set seeds for each sample in the batch\n","    seeds = [random.randrange(2**32) for _ in range(num_samples)]\n","\n","    results = []\n","    try:\n","        # Generate all samples in one batch call\n","        with torch.no_grad():\n","            output_ids = model.generate(\n","                **batch_inputs,\n","                max_new_tokens=256,\n","                temperature=temperature,\n","                do_sample=True,\n","                top_p=0.95,\n","                pad_token_id=tokenizer.eos_token_id or tokenizer.pad_token_id,\n","            )\n","\n","        # Process each output\n","        for i in range(num_samples):\n","            decoded = tokenizer.decode(output_ids[i], skip_special_tokens=True)\n","            response = decoded.split(\"<|assistant|>\")[-1].strip()\n","\n","            answer = extract_numeric_answer(response)\n","            reasoning = extract_reasoning(response)\n","\n","            results.append((answer, reasoning))\n","\n","    except Exception as e:\n","        print(f\"Batch generation failed: {e}\")\n","        # Fallback to individual generation\n","        return generate_multiple_answers_fallback(question, num_samples, temperature)\n","\n","    return results\n","\n","def generate_multiple_answers_fallback(question: str, num_samples: int = 3, temperature: float = 0.7):\n","    prompt = format_prompt(question)\n","    results = []\n","\n","    for i in range(num_samples):\n","        try:\n","            seed = random.randrange(2**32)\n","            torch.manual_seed(seed)\n","\n","            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","            with torch.no_grad():\n","                output_ids = model.generate(\n","                    **inputs,\n","                    max_new_tokens=256,\n","                    temperature=temperature,\n","                    do_sample=True,\n","                    top_p=0.95,\n","                    pad_token_id=tokenizer.eos_token_id or tokenizer.pad_token_id,\n","                )\n","\n","            decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","            response = decoded.split(\"<|assistant|>\")[-1].strip()\n","\n","            answer = extract_numeric_answer(response)\n","            reasoning = extract_reasoning(response)\n","\n","            results.append((answer, reasoning))\n","\n","        except Exception as e:\n","            print(f\"Generation {i+1} failed: {e}\")\n","            results.append((None, \"\"))\n","\n","    return results"],"metadata":{"id":"W_ciNkcQo9Rn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Metric calculations\n","def get_majority_answer(results):\n","    valid_results = [(ans, reasoning) for ans, reasoning in results if ans is not None]\n","\n","    if not valid_results:\n","        return None\n","\n","    answers = [ans for ans, _ in valid_results]\n","    answer_counts = Counter(answers)\n","    majority_answer, majority_count = answer_counts.most_common(1)[0]\n","\n","    return majority_answer\n","\n","def check_accuracy(pred_answer: str, ground_truth: str) -> bool:\n","    if pred_answer is None or ground_truth is None:\n","        return False\n","\n","    try:\n","        pred_num = float(pred_answer)\n","        gt_num = float(ground_truth)\n","        return abs(pred_num - gt_num) < 1e-6  # Floating point precision\n","    except ValueError:\n","        return pred_answer.strip() == ground_truth.strip()\n","\n","def calculate_consistency_ratio(results):\n","    valid_results = [(ans, reasoning) for ans, reasoning in results if ans is not None]\n","\n","    if not valid_results:\n","        return 0.0\n","\n","    answers = [ans for ans, _ in valid_results]\n","    answer_counts = Counter(answers)\n","    majority_count = answer_counts.most_common(1)[0][1]\n","\n","    return majority_count / len(valid_results)\n","\n","def calculate_nli_faithfulness(reasoning: str, answer: str) -> float:\n","    try:\n","        if not reasoning or not answer:\n","            return 0.0\n","\n","        hypothesis = f\"The final answer is {answer}.\"\n","        premise = reasoning\n","\n","        inputs = nli_tokenizer(\n","            premise, hypothesis,\n","            return_tensors=\"pt\",\n","            truncation=True,\n","            max_length=512\n","        ).to(nli_model.device)\n","\n","        with torch.no_grad():\n","            logits = nli_model(**inputs).logits\n","            probs = logits.softmax(dim=-1)\n","\n","        return probs[0][2].item()  # Entailment probability\n","\n","    except:\n","        return 0.0"],"metadata":{"id":"3d36BYp1pFSM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_dataset(df, num_samples=3, temperature=0.7):\n","    consistency_ratios = []\n","    nli_scores = []\n","    sc_accuracy_scores = []\n","    greedy_accuracy_scores = []\n","\n","    print(f\"Evaluating {len(df)} questions...\")\n","\n","    for i, (_, sample) in enumerate(tqdm(df.iterrows(), total=len(df))):\n","        question = sample[\"question\"]\n","        ground_truth = extract_hash_answer(sample[\"answer\"])\n","\n","        # Generate multiple reasoning paths\n","        results = generate_multiple_answers_batch(question, num_samples, temperature)\n","\n","        majority_answer = get_majority_answer(results)\n","\n","        first_answer = results[0][0] if results else None\n","\n","        # Consistency Ratio\n","        consistency_ratio = calculate_consistency_ratio(results)\n","        consistency_ratios.append(consistency_ratio)\n","\n","        # Self-Consistency Accuracy\n","        sc_is_correct = check_accuracy(majority_answer, ground_truth)\n","        sc_accuracy_scores.append(1.0 if sc_is_correct else 0.0)\n","\n","        # Greedy Accuracy\n","        greedy_is_correct = check_accuracy(first_answer, ground_truth)\n","        greedy_accuracy_scores.append(1.0 if greedy_is_correct else 0.0)\n","\n","        # NLI Faithfulness\n","        nli_score = 0.0\n","        if majority_answer:\n","            for ans, reasoning in results:\n","                if ans == majority_answer and reasoning:\n","                    nli_score = calculate_nli_faithfulness(reasoning, majority_answer)\n","                    break\n","        nli_scores.append(nli_score)\n","\n","        print(f\"Q{i+1}: CR={consistency_ratio:.3f}, NLI={nli_score:.3f}, SC-ACC={1 if sc_is_correct else 0}, G-ACC={1 if greedy_is_correct else 0}\")\n","\n","    return consistency_ratios, nli_scores, sc_accuracy_scores, greedy_accuracy_scores"],"metadata":{"id":"KovRUNU5pQN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gsm8k = pd.read_parquet(\"/content/test-00000-of-00001.parquet\") # GSM8K test set downloaded as parquet\n","test_data = gsm8k.sample(n=200, random_state=42).reset_index(drop=True)\n","\n","import time\n","start_time = time.time()\n","\n","consistency_ratios, nli_scores, sc_accuracy_scores, greedy_accuracy_scores = evaluate_dataset(\n","    test_data,\n","    num_samples=3,\n","    temperature=0.7\n",")\n","\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","elapsed_minutes = elapsed_time / 60\n","\n","print(\"FINAL RESULTS\")\n","print(f\"Consistency Ratio: {np.mean(consistency_ratios):.3f}\")\n","print(f\"NLI Faithfulness: {np.mean(nli_scores):.3f}\")\n","print(f\"Self-Consistency Accuracy: {np.mean(sc_accuracy_scores):.3f}\")\n","print(f\"Greedy Accuracy: {np.mean(greedy_accuracy_scores):.3f}\")\n","print(f\"Evaluation Time: {elapsed_minutes:.1f} minutes\")"],"metadata":{"id":"mY2ugYHlpTfd"},"execution_count":null,"outputs":[]}]}
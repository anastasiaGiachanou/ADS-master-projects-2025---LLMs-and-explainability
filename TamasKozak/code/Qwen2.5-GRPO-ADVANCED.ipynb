{"cells":[{"cell_type":"markdown","metadata":{"id":"a0SFs7OjBBFF"},"source":["### Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31j3_ygpBBFG"},"outputs":[],"source":["%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth vllm\n","else:\n","    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n","    !pip install --no-deps unsloth vllm==0.8.5.post1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTNeh2PcBBFG"},"outputs":[],"source":["#@title Colab Extra Install { display-mode: \"form\" }\n","%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth vllm\n","else:\n","    !pip install --no-deps unsloth vllm==0.8.5.post1\n","    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n","    # Skip restarting message in Colab\n","    import sys, re, requests; modules = list(sys.modules.keys())\n","    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n","    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n","\n","    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n","    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n","    with open(\"vllm_requirements.txt\", \"wb\") as file:\n","        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n","    !pip install -r vllm_requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"ZkH_y8UC9lvv"},"source":["### Unsloth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DkIvEkIIkEyB"},"outputs":[],"source":["from unsloth import FastLanguageModel\n","import torch\n","max_seq_length = 1024 # Can increase for longer reasoning traces\n","lora_rank = 16 # Larger rank = smarter, but slower\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"Qwen/Qwen2.5-14B-Instruct\",\n","    max_seq_length = max_seq_length,\n","    load_in_4bit = True, # False for LoRA 16bit\n","    fast_inference = True, # Enable vLLM fast inference\n","    max_lora_rank = lora_rank\n",")\n","\n","model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\n","        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","        \"gate_proj\", \"up_proj\", \"down_proj\",\n","    ],\n","    lora_alpha = lora_rank * 2, # *2 speeds up training\n","    use_gradient_checkpointing = True, # Reduces memory usage\n","    random_state = 3407,\n",")"]},{"cell_type":"markdown","metadata":{"id":"W9DuiVRLhMco"},"source":["### GRPO chat template\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6UjowCbT-cFz"},"outputs":[],"source":["reasoning_start = \"<start_working_out>\"\n","reasoning_end   = \"<end_working_out>\"\n","solution_start  = \"<SOLUTION>\"\n","solution_end    = \"</SOLUTION>\"\n","\n","system_prompt = \\\n","f\"\"\"You are given a problem.\n","Think about the problem and provide your working out.\n","Place it between {reasoning_start} and {reasoning_end}.\n","Then, provide your solution between {solution_start}{solution_end}\"\"\"\n","system_prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3fF9gMujY02"},"outputs":[],"source":["chat_template = \\\n","    \"{% if messages[0]['role'] == 'system' %}\"\\\n","        \"{{ messages[0]['content'] + eos_token }}\"\\\n","        \"{% set loop_messages = messages[1:] %}\"\\\n","    \"{% else %}\"\\\n","        \"{{ '{system_prompt}' + eos_token }}\"\\\n","        \"{% set loop_messages = messages %}\"\\\n","    \"{% endif %}\"\\\n","    \"{% for message in loop_messages %}\"\\\n","        \"{% if message['role'] == 'user' %}\"\\\n","            \"{{ message['content'] }}\"\\\n","        \"{% elif message['role'] == 'assistant' %}\"\\\n","            \"{{ message['content'] + eos_token }}\"\\\n","        \"{% endif %}\"\\\n","    \"{% endfor %}\"\\\n","    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n","    \"{% endif %}\"\n","\n","# Replace with out specific template:\n","chat_template = chat_template\\\n","    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n","    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n","tokenizer.chat_template = chat_template"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BciEDYSSYFNj"},"outputs":[],"source":["tokenizer.apply_chat_template([\n","    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n","    {\"role\" : \"assistant\", \"content\" : f\"{reasoning_start}I think it's 2.{reasoning_end}{solution_start}2{solution_end}\"},\n","    {\"role\" : \"user\", \"content\" : \"What is 2+2?\"},\n","], tokenize = False, add_generation_prompt = True)"]},{"cell_type":"markdown","metadata":{"id":"_mdsuGjxHrjT"},"source":["### Pre fine-tuning for formatting\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXxM2lStVIkd"},"outputs":[],"source":["from datasets import load_dataset\n","import pandas as pd\n","import numpy as np\n","\n","dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n","dataset = dataset.to_pandas()[\n","    [\"answer\", \"question\"]\n","]\n","\n","# Extract numeric answer from the answer field (GSM8K answers are in format \"#### 123\")\n","def extract_numeric_answer(answer_text):\n","    if \"####\" in answer_text:\n","        return answer_text.split(\"####\")[-1].strip()\n","    return answer_text\n","\n","dataset[\"expected_answer\"] = dataset[\"answer\"].apply(extract_numeric_answer)\n","dataset[\"problem\"] = dataset[\"question\"]\n","\n","# Try converting to number - if not, replace with NaN\n","is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors=\"coerce\").notnull()\n","# Select only numbers\n","dataset = dataset.iloc[np.where(is_number)[0]]\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"JVRFqoSdIEVK"},"source":["We have to format the dataset to follow our GRPO style formatting:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z9ydcV_Abfi6"},"outputs":[],"source":["def format_dataset(x):\n","    expected_answer = x[\"expected_answer\"]\n","    problem = x[\"problem\"]\n","\n","    # For GSM8K, we need to extract the reasoning from the full answer\n","    # GSM8K format: reasoning text followed by \"#### [number]\"\n","    full_answer = x[\"answer\"]\n","    if \"####\" in full_answer:\n","        thoughts = full_answer.split(\"####\")[0].strip()\n","    else:\n","        thoughts = full_answer.strip()\n","\n","    # Add our custom formatting\n","    final_prompt = \\\n","        reasoning_start + thoughts + reasoning_end + \\\n","        solution_start + expected_answer + solution_end\n","\n","    return [\n","        {\"role\" : \"system\",    \"content\" : system_prompt},\n","        {\"role\" : \"user\",      \"content\" : problem},\n","        {\"role\" : \"assistant\", \"content\" : final_prompt},\n","    ]\n","\n","dataset[\"Messages\"] = dataset.apply(format_dataset, axis = 1)"]},{"cell_type":"markdown","metadata":{"id":"X5NI47rOIRP2"},"source":["Check to see if it worked:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTdXBKcslhRH"},"outputs":[],"source":["tokenizer.apply_chat_template(dataset[\"Messages\"][0], tokenize = False)"]},{"cell_type":"markdown","metadata":{"id":"iHV9BXYiIYaq"},"source":["Truncate the pre fine-tuning dataset to `max_seq_length/2` since we don't want too long reasoning traces."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBHFlRbae9_s"},"outputs":[],"source":["dataset[\"N\"] = dataset[\"Messages\"].apply(lambda x: len(tokenizer.apply_chat_template(x)))\n","\n","dataset = dataset.loc[dataset[\"N\"] <= max_seq_length/2].copy()\n","dataset.shape"]},{"cell_type":"markdown","source":["Keep only 78 examples"],"metadata":{"id":"M41V5tHzFID6"}},{"cell_type":"code","source":["dataset = dataset.sample(n=78, random_state=42)\n","dataset.shape"],"metadata":{"id":"tkPnRBPQFDSS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E6NkUCAGIj8N"},"source":["We then tokenize the messages and convert it to a Hugging Face compatible dataset format:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3rgdtiV_f5hx"},"outputs":[],"source":["from datasets import Dataset\n","\n","dataset[\"text\"] = tokenizer.apply_chat_template(dataset[\"Messages\"].values.tolist(), tokenize = False)\n","dataset = Dataset.from_pandas(dataset)\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"bAQJjQrYKzOk"},"source":["Let's now pre fine-tune the model so it follows our custom GRPO formatting!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"woYi0SSygpqp"},"outputs":[],"source":["from trl import SFTTrainer, SFTConfig\n","trainer = SFTTrainer(\n","    model = model,\n","    tokenizer = tokenizer,\n","    train_dataset = dataset,\n","    args = SFTConfig(\n","        dataset_text_field = \"text\",\n","        per_device_train_batch_size = 1,\n","        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n","        warmup_steps = 5,\n","        num_train_epochs = 2, # Set this for 1 full training run.\n","        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n","        logging_steps = 5,\n","        optim = \"adamw_8bit\",\n","        weight_decay = 0.01,\n","        lr_scheduler_type = \"linear\",\n","        seed = 3407,\n","        report_to = \"none\", # Use this for WandB etc\n","    ),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4-2v_bLhZuE"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"DRMBNUBgLC8T"},"source":["Let's check if the model has learnt to follow the custom format:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HJxrS76h3Ds"},"outputs":[],"source":["text = tokenizer.apply_chat_template(\n","    dataset[0][\"Messages\"][:2],\n","    tokenize = False,\n","    add_generation_prompt = True, # Must add for generation\n",")\n","\n","from transformers import TextStreamer\n","_ = model.generate(\n","    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n","    temperature = 0.01,\n","    max_new_tokens = 1024,\n","    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",")"]},{"cell_type":"markdown","metadata":{"id":"AtZ3qGOALF95"},"source":["Yes it did follow the formatting! Great! Let's remove some items before the GRPO step"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YWSZ0DET7bob"},"outputs":[],"source":["del dataset\n","torch.cuda.empty_cache()\n","import gc\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"7KGgPgk_5S8r"},"source":["### Data Prep\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7-eUrQn-OzE"},"outputs":[],"source":["from datasets import load_dataset\n","dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n","\n","dataset = dataset.train_test_split(train_size=0.15, seed=42)[\"train\"]\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"3b00gUsS-ROW"},"source":["Let's look at the first row:"]},{"cell_type":"code","source":["# Create evaluation dataset from a portion of your 15% dataset\n","eval_split = dataset.train_test_split(test_size=0.1, seed=42)  # 10% for eval, 90% for train\n","train_dataset = eval_split[\"train\"]\n","eval_dataset = eval_split[\"test\"]\n","\n","print(f\"Train dataset size: {len(train_dataset)}\")\n","print(f\"Eval dataset size: {len(eval_dataset)}\")"],"metadata":{"id":"qAsPfKvNJU9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"siopxjG8-ReF"},"outputs":[],"source":["dataset[0][\"question\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KGupRQqD-Wcf"},"outputs":[],"source":["dataset[0][\"answer\"]"]},{"cell_type":"markdown","metadata":{"id":"CmnXj6hn-Ydi"},"source":["In GSM8K, we notice all answers like about have a ####, so we extract it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JJGXKdJ-Zl_"},"outputs":[],"source":["def extract_hash_answer(text):\n","    # if \"####\" not in text: return None\n","    return text.split(\"####\")[1].strip()\n","    #return text\n","extract_hash_answer(dataset[0][\"answer\"])"]},{"cell_type":"markdown","metadata":{"id":"K30CygaU-dir"},"source":["Let's map the dataset! and see the first row:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qyEVI972-d3n"},"outputs":[],"source":["# Apply the mapping to both datasets\n","train_dataset = train_dataset.map(lambda x: {\n","    \"prompt\": [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": x[\"question\"]},\n","    ],\n","    \"answer\": extract_hash_answer(x[\"answer\"]),\n","})\n","\n","eval_dataset = eval_dataset.map(lambda x: {\n","    \"prompt\": [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": x[\"question\"]},\n","    ],\n","    \"answer\": extract_hash_answer(x[\"answer\"]),\n","})\n","\n","# Check format\n","print(\"Train dataset example:\")\n","print(train_dataset[0])\n","print(\"\\nEval dataset example:\")\n","print(eval_dataset[0])"]},{"cell_type":"markdown","metadata":{"id":"-9m8eR9T-gMh"},"source":["We create a regex format to match the reasoning sections and answers:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQwjTjNz-gY_"},"outputs":[],"source":["import re\n","\n","# Add optional EOS token matching\n","solution_end_regex = r\"</SOLUTION>[\\s]{0,}\" + \\\n","    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n","\n","match_format = re.compile(\n","    rf\"{reasoning_end}.*?\"\\\n","    rf\"{solution_start}(.+?){solution_end_regex}\"\\\n","    rf\"[\\s]{{0,}}$\",\n","    flags = re.MULTILINE | re.DOTALL\n",")\n","match_format"]},{"cell_type":"markdown","metadata":{"id":"OycMneOq-iNC"},"source":["We verify it works:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndzHnQ_6-jHt"},"outputs":[],"source":["match_format.findall(\n","    \"Let me think!<end_working_out>\"\\\n","    f\"<SOLUTION>\\n2\\n</SOLUTION>\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eRMDAzDk2x6t"},"outputs":[],"source":["match_format.findall(\n","    \"<start_working_out>Let me think!<end_working_out>\"\\\n","    f\"<SOLUTION>  2  </SOLUTION>\\n\\n\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"weOjmO5l-kl3"},"source":["We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgFNXORy-lpO"},"outputs":[],"source":["def match_format_exactly(completions, **kwargs):\n","    scores = []\n","    for completion in completions:\n","        score = 0\n","        response = completion[0][\"content\"]\n","        # Match if format is seen exactly!\n","        if match_format.search(response) is not None: score += 3.0\n","        scores.append(score)\n","    return scores"]},{"cell_type":"markdown","metadata":{"id":"Gf69i2WT-m4K"},"source":["If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUfHzCVx-nGK"},"outputs":[],"source":["def match_format_approximately(completions, **kwargs):\n","    scores = []\n","    for completion in completions:\n","        score = 0\n","        response = completion[0][\"content\"]\n","        # Count how many keywords are seen - we penalize if too many!\n","        # If we see 1, then plus some points!\n","\n","        # No need to reward <start_working_out> since we always prepend it!\n","        # score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n","        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n","        score += 0.5 if response.count(solution_start)  == 1 else -1.0\n","        score += 0.5 if response.count(solution_end)    == 1 else -1.0\n","        scores.append(score)\n","    return scores"]},{"cell_type":"markdown","metadata":{"id":"9wAUWwtE-s6n"},"source":["Finally, we want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmtI_8gg-uIE"},"outputs":[],"source":["def check_answer(prompts, completions, answer, **kwargs):\n","    question = prompts[0][-1][\"content\"]\n","    responses = [completion[0][\"content\"] for completion in completions]\n","\n","    extracted_responses = [\n","        guess.group(1)\n","        if (guess := match_format.search(r)) is not None else None \\\n","        for r in responses\n","    ]\n","\n","    scores = []\n","    for guess, true_answer in zip(extracted_responses, answer):\n","        score = 0\n","        if guess is None:\n","            scores.append(-2.0)\n","            continue\n","        # Correct answer gets 5 points!\n","        if guess == true_answer:\n","            score += 5.0\n","        # Match if spaces are seen, but less reward\n","        elif guess.strip() == true_answer.strip():\n","            score += 3.5\n","        else:\n","            # We also reward it if the answer is close via ratios!\n","            # Ie if the answer is within some range, reward it!\n","            try:\n","                ratio = float(guess) / float(true_answer)\n","                if   ratio >= 0.9 and ratio <= 1.1: score += 2.0\n","                elif ratio >= 0.8 and ratio <= 1.2: score += 1.5\n","                else: score -= 2.5 # Penalize wrong answers\n","            except:\n","                score -= 4.5 # Penalize\n","        scores.append(score)\n","    return scores"]},{"cell_type":"markdown","metadata":{"id":"atMyfhXh-v3R"},"source":["Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20.\n","\n","We also remove possible commas for example as in 123,456"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVW0kL8q-wL5"},"outputs":[],"source":["match_numbers = re.compile(\n","    solution_start + r\".*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\",\n","    flags = re.MULTILINE | re.DOTALL\n",")\n","print(match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\"))\n","print(match_numbers.findall(\"<SOLUTION>  123,456  </SOLUTION>\"))\n","print(match_numbers.findall(\"<SOLUTION>  -0.234  </SOLUTION>\"))\n","print(match_numbers.findall(\"<SOLUTION>17</SOLUTION>\"))"]},{"cell_type":"markdown","metadata":{"id":"RbfaaAywNHHh"},"source":["We now prepare our main function which will print out the generated responses and the true answer, along with another reward function which converts text to float via `float` and sees if it's the same."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GjBFrttr-y1_"},"outputs":[],"source":["global PRINTED_TIMES\n","PRINTED_TIMES = 0\n","global PRINT_EVERY_STEPS\n","PRINT_EVERY_STEPS = 5\n","\n","def check_numbers(prompts, completions, answer, **kwargs):\n","    question = prompts[0][-1][\"content\"]\n","    responses = [completion[0][\"content\"] for completion in completions]\n","\n","    extracted_responses = [\n","        guess.group(1)\n","        if (guess := match_numbers.search(r)) is not None else None \\\n","        for r in responses\n","    ]\n","\n","    scores = []\n","    # Print only every few steps\n","    global PRINTED_TIMES\n","    global PRINT_EVERY_STEPS\n","    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n","        print(\n","            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n","        )\n","    PRINTED_TIMES += 1\n","\n","    for guess, true_answer in zip(extracted_responses, answer):\n","        if guess is None:\n","            scores.append(-2.5)\n","            continue\n","        # Convert to numbers\n","        try:\n","            true_answer = float(true_answer.strip())\n","            # Remove commas like in 123,456\n","            guess       = float(guess.strip().replace(\",\", \"\"))\n","            scores.append(3.5 if guess == true_answer else -1.5)\n","        except:\n","            scores.append(0)\n","            continue\n","    return scores"]},{"cell_type":"markdown","metadata":{"id":"fgOR3wJ_AyLr"},"source":["Get the top 90% prompt length so we don't accidentally truncate them!\n","\n","We'll remove the top 10% long prompts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6EgAi4Q5fGE-"},"outputs":[],"source":["# Filter train_dataset\n","tokenized_train = train_dataset.map(\n","    lambda x: {\"tokens\": tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt=True, tokenize=True)}\n",")\n","tokenized_train = tokenized_train.map(lambda x: {\"L\": len(x[\"tokens\"])})\n","\n","# Filter eval_dataset\n","tokenized_eval = eval_dataset.map(\n","    lambda x: {\"tokens\": tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt=True, tokenize=True)}\n",")\n","tokenized_eval = tokenized_eval.map(lambda x: {\"L\": len(x[\"tokens\"])})\n","\n","# Use the same maximum_length threshold for both\n","import numpy as np\n","# You can recalculate or reuse the previous maximum_length\n","maximum_length = int(np.quantile(tokenized_train[\"L\"], 0.9))  # Or use your previous value: 136\n","print(\"Max Length = \", maximum_length)\n","\n","# Filter both datasets\n","train_dataset = train_dataset.select(np.where(np.array(tokenized_train[\"L\"]) <= maximum_length)[0])\n","eval_dataset = eval_dataset.select(np.where(np.array(tokenized_eval[\"L\"]) <= maximum_length)[0])\n","\n","# Clean up\n","del tokenized_train, tokenized_eval\n","\n","print(f\"Filtered train dataset size: {len(train_dataset)}\")\n","print(f\"Filtered eval dataset size: {len(eval_dataset)}\")"]},{"cell_type":"markdown","metadata":{"id":"9-IOMhVg-2AM"},"source":["<a name=\"Train\"></a>\n","### Train the model\n","\n","Now set up GRPO Trainer and all configurations!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptqkXK2D4d6p"},"outputs":[],"source":["max_prompt_length = maximum_length + 1 # + 1 just in case!\n","max_completion_length = max_seq_length - max_prompt_length\n","\n","from trl import GRPOConfig, GRPOTrainer\n","from vllm import SamplingParams\n","\n","vllm_sampling_params = SamplingParams(\n","    min_p = 0.1,\n","    top_p = 1.0,\n","    top_k = -1,\n","    seed = 3407,\n","    stop = [tokenizer.eos_token],\n","    include_stop_str_in_output = True,\n",")\n","\n","training_args = GRPOConfig(\n","    vllm_sampling_params = vllm_sampling_params,\n","    temperature = 1.0,\n","    learning_rate = 5e-6,\n","    weight_decay = 0.01,\n","    warmup_ratio = 0.1,\n","    lr_scheduler_type = \"linear\",\n","    optim = \"adamw_8bit\",\n","\n","    logging_steps = 2,\n","    logging_first_step = True,\n","\n","    per_device_train_batch_size = 1,\n","    gradient_accumulation_steps = 4,\n","    num_generations = 4,\n","    max_prompt_length = max_prompt_length,\n","    max_completion_length = max_completion_length,\n","    num_train_epochs = 1,\n","    #max_steps = 172,\n","\n","    # Exactly 4 checkpoints at 25%, 50%, 75%, 100%\n","    save_steps = 57,\n","    eval_steps = 57,\n","\n","    report_to = \"none\", # Can use wandb\n","    output_dir = \"outputs_14B\", # Change based on model\n","\n","    per_device_eval_batch_size = 4,\n","    eval_accumulation_steps = 1,\n","    eval_strategy = \"steps\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzOuSVCL_GA9"},"outputs":[],"source":["trainer = GRPOTrainer(\n","    model = model,\n","    processing_class = tokenizer,\n","    reward_funcs = [\n","        match_format_exactly,\n","        match_format_approximately,\n","        check_answer,\n","        check_numbers,\n","    ],\n","    args = training_args,\n","    train_dataset = train_dataset,\n","    eval_dataset = eval_dataset\n",")\n","\n","trainer.train()"]},{"cell_type":"code","source":["import shutil\n","import os\n","from google.colab import files\n","from zipfile import ZipFile\n","\n","def download_lora_checkpoint(checkpoint_folder):\n","    zip_filename = os.path.basename(checkpoint_folder) + \"_grpo.zip\"\n","\n","    files_to_keep = [\n","        \"adapter_model.safetensors\",\n","        \"adapter_config.json\",\n","        \"tokenizer.json\",\n","        \"tokenizer_config.json\",\n","        \"vocab.json\",\n","        \"merges.txt\",\n","        \"special_tokens_map.json\",\n","        \"added_tokens.json\",\n","        \"chat_template.jinja\",\n","        \"trainer_state.json\",\n","        \"training_args.bin\",\n","    ]\n","\n","    with ZipFile(zip_filename, 'w') as zipf:\n","        for file_name in files_to_keep:\n","            file_path = os.path.join(checkpoint_folder, file_name)\n","            if os.path.exists(file_path):\n","                zipf.write(file_path, os.path.join(os.path.basename(checkpoint_folder), file_name))\n","\n","    files.download(zip_filename)\n","\n","download_lora_checkpoint(\"outputs_14B/checkpoint-171\") # Select the best checkpoint"],"metadata":{"id":"1fDFYN6XNeTc"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/unslothai/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb","timestamp":1750115565990}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}